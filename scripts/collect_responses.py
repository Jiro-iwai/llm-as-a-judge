#!/usr/bin/env python3
"""
LLM Response Collector Script

This script collects responses from the LLM API for two models (claude3.5-sonnet and claude4.5-haiku)
and creates a CSV file suitable for evaluation.

Usage:
    python collect_responses.py questions.txt -o output.csv
    python collect_responses.py questions.txt --api-url http://localhost:8080/api/v1/urls
"""

import argparse
import csv
import json
import os
import sys
import time
import uuid
from pathlib import Path
from typing import List, Optional

# Add project root to Python path (must be before other imports)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd  # noqa: E402
import requests  # noqa: E402
from tqdm import tqdm  # noqa: E402

from compare_processing_time import (
    create_comparison_chart,
    create_statistics_chart,
    create_summary_table,
    extract_processing_times,
)
from src.utils.logging_config import (
    log_info,
    log_error,
    log_warning,
    log_success,
    log_section,
    setup_logging,
)
from src.utils.log_output_simplifier import (
    clean_and_format_llm_log,
)
from src.config.app_config import (
    get_timeout,
    get_api_delay,
    get_default_identity,
    get_output_file_names,
)

# Set up logging system
setup_logging()


def initialize_processing_time_log(log_file: str) -> None:
    """
    Initialize (or clear) the processing time log file.
    """
    if not log_file:
        return
    log_path = Path(log_file)
    try:
        if log_path.parent and log_path.parent != Path(""):
            log_path.parent.mkdir(parents=True, exist_ok=True)
        with log_path.open("w", encoding="utf-8") as file:
            file.write("# Processing time log generated by collect_responses.py\n")
    except OSError as exc:
        log_warning(
            f"å‡¦ç†æ™‚é–“ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸ: {exc}. ãƒ­ã‚°å‡ºåŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )


def log_processing_time_entry(
    model_name: str,
    elapsed_time: float,
    log_file: Optional[str],
    question_number: Optional[int] = None,
    model_label: Optional[str] = None,
) -> None:
    """
    Append a standardized processing time entry for downstream comparison.
    """
    if not log_file:
        return

    label = model_label or ""
    question_part = f" è³ªå•{question_number}" if question_number is not None else ""
    line = f"ğŸ“¥ [{model_name}] {label}{question_part} çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’"

    try:
        with open(log_file, "a", encoding="utf-8") as file:
            file.write(line + os.linesep)
    except OSError as exc:
        log_warning(f"å‡¦ç†æ™‚é–“ãƒ­ã‚°ã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({log_file}): {exc}")


def format_response(response_text: str) -> str:
    """
    Format the API response using the log simplifier.

    Args:
        response_text: The raw response text from API (answer field content)

    Returns:
        Formatted response string
    """
    if not response_text:
        return ""

    # Wrap the response in a JSON-like structure for the formatter
    # The formatter expects JSON with an "answer" field, but we already have the answer content
    try:
        # Try to format as if it's already the answer content
        formatted = clean_and_format_llm_log(response_text)
        return formatted
    except (ValueError, KeyError, AttributeError) as e:
        # Handle specific errors during log formatting
        log_warning(f"ãƒ­ã‚°æ•´å½¢ã‚¨ãƒ©ãƒ¼ ({type(e).__name__}): {e}")
        return response_text
    except Exception as e:
        # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªæƒ…å ±ã‚’è¨˜éŒ²
        log_warning(f"äºˆæœŸã—ãªã„ãƒ­ã‚°æ•´å½¢ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        return response_text


def call_api(
    question: str,
    api_url: str,
    model_name: str,
    identity: Optional[str] = None,
    timeout: Optional[int] = None,
    verbose: bool = True,
    time_log_path: Optional[str] = None,
    question_number: Optional[int] = None,
    model_label: Optional[str] = None,
) -> Optional[str]:
    """
    Call the LLM API and return the response.

    Args:
        question: The question to ask
        api_url: The API endpoint URL
        model_name: The model name (claude3.5-sonnet or claude4.5-haiku)
        identity: The x-amzn-oidc-identity header value (defaults to config value)
        timeout: Request timeout in seconds (defaults to config value)
        verbose: Whether to print detailed logs
        time_log_path: Optional path to a log file to record processing times
        question_number: Optional question index (1-based) for logging
        model_label: Optional label (e.g., "Model A") for logging

    Returns:
        The response text, or None if failed
    """
    # Use config values if not provided
    if identity is None:
        identity = get_default_identity()
    if timeout is None:
        timeout = get_timeout()

    question_uuid = str(uuid.uuid4())

    # Prepare the request
    url = f"{api_url}?llm_model_name={model_name}&rag_enabled=auto"
    headers = {"x-amzn-oidc-identity": identity, "Content-Type": "application/json"}
    data = {
        "question_uuid": question_uuid,
        "messages": [{"role": "user", "content": question}],
    }

    if verbose:
        log_info(f"ğŸ“¤ [{model_name}] APIå‘¼ã³å‡ºã—é–‹å§‹", indent=1)
        log_info(f"URL: {url}", indent=2)
        question_preview = question[:60] + "..." if len(question) > 60 else question
        log_info(f"è³ªå•: {question_preview}", indent=2)

    start_time = time.time()
    response: Optional[requests.Response] = None

    try:
        response = requests.post(url, headers=headers, json=data, timeout=timeout)
        elapsed_time = time.time() - start_time

        if verbose:
            log_info(
                f"ğŸ“¥ [{model_name}] HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code} (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
                indent=1,
            )

        response.raise_for_status()

        # Parse response - API returns object format: {"answer": "...", "urls": [...], ...}
        response_data = response.json()

        # Handle both array and object formats
        if isinstance(response_data, list) and len(response_data) >= 1:
            # Array format: ["{...}", status_code]
            json_str = response_data[0]
            if isinstance(json_str, str):
                parsed = json.loads(json_str)
            else:
                parsed = json_str
        elif isinstance(response_data, dict):
            # Direct object format: {"answer": "...", ...}
            parsed = response_data
        else:
            if verbose:
                log_warning(f"[{model_name}] äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼", indent=1)
            return response.text

        # Extract the answer field (this contains the LLM response)
        if "answer" in parsed:
            answer = parsed["answer"]
            answer_length = len(answer)
            if verbose:
                log_success(
                    f"[{model_name}] ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—æˆåŠŸ (answeré•·ã•: {answer_length:,}æ–‡å­—)",
                    indent=1,
                )
                if "urls" in parsed and isinstance(parsed["urls"], list):
                    log_info(f"æ¤œç´¢çµæœURLæ•°: {len(parsed['urls'])}", indent=2)
            log_processing_time_entry(
                model_name=model_name,
                elapsed_time=elapsed_time,
                log_file=time_log_path,
                question_number=question_number,
                model_label=model_label,
            )
            return answer
        else:
            if verbose:
                log_warning(
                    f"[{model_name}] 'answer'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", indent=1
                )
            return response.text

    except requests.exceptions.Timeout:
        elapsed_time = time.time() - start_time
        log_error(
            f"[{model_name}] APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            indent=1,
        )
        log_error(f"URL: {url}", indent=2)
        return None
    except requests.exceptions.RequestException as e:
        elapsed_time = time.time() - start_time
        log_error(
            f"[{model_name}] APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            indent=1,
        )
        log_error(f"ã‚¨ãƒ©ãƒ¼: {e}", indent=2)
        log_error(f"URL: {url}", indent=2)
        return None
    except (json.JSONDecodeError, KeyError) as e:
        elapsed_time = time.time() - start_time
        log_error(
            f"[{model_name}] ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            indent=1,
        )
        log_error(f"ã‚¨ãƒ©ãƒ¼: {e}", indent=2)
        response_text: Optional[str] = None
        if response is not None:
            try:
                response_text = (
                    response.text[:200]
                    if hasattr(response, "text")
                    else str(response)[:200]
                )
                log_info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {response_text}", indent=2)
            except AttributeError:
                pass
        return response_text


def collect_responses(
    questions: List[str],
    api_url: str,
    model_a: str,
    model_b: str,
    identity: Optional[str] = None,
    timeout: Optional[int] = None,
    delay: Optional[float] = None,
    verbose: bool = True,
    time_log_path: Optional[str] = None,
) -> pd.DataFrame:
    """
    Collect responses from both models for all questions.

    Args:
        questions: List of questions to ask
        api_url: The API endpoint URL
        model_a: Model name for Model A (e.g., claude3.5-sonnet)
        model_b: Model name for Model B (e.g., claude4.5-haiku)
        identity: The x-amzn-oidc-identity header value (defaults to config value)
        timeout: Request timeout in seconds (defaults to config value)
        delay: Delay between API calls in seconds (defaults to config value)
        verbose: Whether to print detailed logs
        time_log_path: Optional path to record processing times for visualization

    Returns:
        DataFrame with Question, Model_A_Response, Model_B_Response columns
        (Each response contains only the "answer" field from API)
    """
    # Use config values if not provided
    if identity is None:
        identity = get_default_identity()
    if timeout is None:
        timeout = get_timeout()
    if delay is None:
        delay = get_api_delay()

    if time_log_path:
        initialize_processing_time_log(time_log_path)

    results = []
    total_start_time = time.time()

    log_section("ğŸ“‹ åé›†è¨­å®š")
    log_info(f"è³ªå•æ•°: {len(questions)}", indent=1)
    log_info(f"Model A: {model_a}", indent=1)
    log_info(f"Model B: {model_b}", indent=1)
    log_info(f"API URL: {api_url}", indent=1)
    log_info(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {delay}ç§’", indent=1)
    log_info(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’", indent=1)
    estimated_time = len(questions) * 2 * (delay + 15)
    log_info(
        f"äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„{estimated_time:.0f}ç§’ (å„APIå‘¼ã³å‡ºã—15ç§’æƒ³å®š)", indent=1
    )
    log_info("")

    success_count_a = 0
    success_count_b = 0
    failed_count_a = 0
    failed_count_b = 0

    for idx, question in enumerate(tqdm(questions, desc="ğŸ“Š é€²æ—"), 1):
        if verbose:
            log_section(f"ğŸ“ è³ªå• {idx}/{len(questions)}")
            log_info(f"è³ªå•: {question}")
            print()

        question_start_time = time.time()

        # Call Model A
        if verbose:
            log_info(f"[{idx}/{len(questions)}] Model A ({model_a}) ã‚’å‘¼ã³å‡ºã—ä¸­...")
        response_a_raw = call_api(
            question,
            api_url,
            model_a,
            identity,
            timeout,
            verbose=verbose,
            time_log_path=time_log_path,
            question_number=idx,
            model_label="Model A",
        )

        # Format the response using log simplifier
        if response_a_raw:
            if verbose:
                log_info("ğŸ”§ Model A ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢ä¸­...", indent=1)
            response_a = format_response(response_a_raw)
            success_count_a += 1
        else:
            response_a = ""
            failed_count_a += 1

        # Wait between Model A and Model B calls
        if verbose:
            log_info(f"â¸ï¸  Model Bå‘¼ã³å‡ºã—ã¾ã§{delay}ç§’å¾…æ©Ÿä¸­...", indent=1)
        time.sleep(delay)  # Rate limiting

        # Call Model B
        if verbose:
            log_info(f"[{idx}/{len(questions)}] Model B ({model_b}) ã‚’å‘¼ã³å‡ºã—ä¸­...")
        response_b_raw = call_api(
            question,
            api_url,
            model_b,
            identity,
            timeout,
            verbose=verbose,
            time_log_path=time_log_path,
            question_number=idx,
            model_label="Model B",
        )

        # Format the response using log simplifier
        if response_b_raw:
            if verbose:
                log_info("ğŸ”§ Model B ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢ä¸­...", indent=1)
            response_b = format_response(response_b_raw)
            success_count_b += 1
        else:
            response_b = ""
            failed_count_b += 1

        # Store formatted responses
        # Column names compatible with both llm_judge_evaluator.py and ragas_llm_judge_evaluator.py
        results.append(
            {
                "Question": question,
                "Model_A_Response": response_a,
                "Model_B_Response": response_b,
            }
        )

        question_elapsed = time.time() - question_start_time

        if verbose:
            status_a = "âœ…" if response_a else "âŒ"
            status_b = "âœ…" if response_b else "âŒ"
            log_info(
                f"\nğŸ“Š è³ªå• {idx} å®Œäº† (çµŒéæ™‚é–“: {question_elapsed:.2f}ç§’)", indent=1
            )
            log_info(f"Model A: {status_a} | Model B: {status_b}", indent=2)
            log_info(
                f"æˆåŠŸæ•°: A={success_count_a}/{idx}, B={success_count_b}/{idx}",
                indent=2,
            )

        # Wait before next question (if not the last question)
        if idx < len(questions):
            if verbose:
                log_info(f"â¸ï¸  æ¬¡ã®è³ªå•ã¾ã§{delay}ç§’å¾…æ©Ÿä¸­...", indent=1)
            time.sleep(delay)  # Rate limiting

    total_elapsed = time.time() - total_start_time

    if verbose:
        log_section("ğŸ“Š åé›†å®Œäº†çµ±è¨ˆ")
        log_info(
            f"ç·å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’ ({total_elapsed / 60:.2f}åˆ†)", indent=1
        )
        log_info(f"è³ªå•æ•°: {len(questions)}", indent=1)
        log_info(f"Model A ({model_a}):", indent=1)
        success_rate_a = (
            success_count_a / len(questions) * 100 if len(questions) > 0 else 0
        )
        log_info(
            f"âœ… æˆåŠŸ: {success_count_a}/{len(questions)} ({success_rate_a:.1f}%)",
            indent=2,
        )
        log_info(f"âŒ å¤±æ•—: {failed_count_a}/{len(questions)}", indent=2)
        log_info(f"Model B ({model_b}):", indent=1)
        success_rate_b = (
            success_count_b / len(questions) * 100 if len(questions) > 0 else 0
        )
        log_info(
            f"âœ… æˆåŠŸ: {success_count_b}/{len(questions)} ({success_rate_b:.1f}%)",
            indent=2,
        )
        log_info(f"âŒ å¤±æ•—: {failed_count_b}/{len(questions)}", indent=2)

    # Generate processing time comparison assets
    generate_processing_time_reports(
        time_log_path, model_a_name=model_a, model_b_name=model_b
    )

    return pd.DataFrame(results)


def read_questions(input_file: str) -> List[str]:
    """
    Read questions from a text file or CSV file.

    Supports:
    - Text file: One question per line
    - CSV file: First column contains questions (with or without header)

    Args:
        input_file: Path to the input file

    Returns:
        List of questions
    """
    questions = []
    try:
        # Check if file is CSV by extension or try to read as CSV first
        if input_file.lower().endswith(".csv"):
            # Try to read as CSV
            df = pd.read_csv(input_file)
            # Get first column (usually "Questions" or "Question")
            first_col = df.columns[0]
            questions = df[first_col].dropna().astype(str).tolist()
            # Remove header if it looks like a header (common header names)
            if questions and questions[0].lower() in [
                "question",
                "questions",
                "q",
                "query",
                "queries",
            ]:
                questions = questions[1:]
            # Filter out empty strings
            questions = [q.strip() for q in questions if q.strip()]
        else:
            # Read as text file (one question per line)
            with open(input_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith(
                        "#"
                    ):  # Skip empty lines and comments
                        questions.append(line)
    except FileNotFoundError:
        log_error(f"Input file '{input_file}' not found.")
        sys.exit(1)
    except PermissionError as e:
        log_error(f"Permission denied accessing file: {e}")
        sys.exit(1)
    except UnicodeDecodeError as e:
        log_error(f"Failed to decode file: {e}")
        log_error("Please check the file encoding (UTF-8 is recommended).")
        sys.exit(1)
    except Exception as e:
        # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªæƒ…å ±ã‚’è¨˜éŒ²
        log_error(
            f"Unexpected error occurred while reading file: {type(e).__name__}: {e}"
        )
        sys.exit(1)

    return questions


def generate_processing_time_reports(
    log_file: Optional[str],
    model_a_name: Optional[str] = None,
    model_b_name: Optional[str] = None,
) -> None:
    """
    Generate comparison charts and summary tables from the processing time log.

    Args:
        log_file: Path to the processing time log file.
        model_a_name: Optional model name for Model A. If provided, uses
            dynamic regex pattern matching.
        model_b_name: Optional model name for Model B. If provided, uses
            dynamic regex pattern matching.
    """
    if not log_file:
        log_warning(
            "å‡¦ç†æ™‚é–“ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )
        return

    log_path = Path(log_file)
    if not log_path.exists():
        log_warning(
            f"å‡¦ç†æ™‚é–“ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« '{log_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )
        return

    try:
        question_numbers, model_a_times, model_b_times = extract_processing_times(
            log_file, model_a_name=model_a_name, model_b_name=model_b_name
        )
    except SystemExit:
        # extract_processing_times already reported an error
        return
    except Exception as exc:
        log_warning(f"å‡¦ç†æ™‚é–“ãƒ­ã‚°ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return

    if not question_numbers:
        log_warning("å‡¦ç†æ™‚é–“ãƒ­ã‚°ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    output_files = get_output_file_names()
    comparison_path = output_files.get(
        "processing_time_comparison", "processing_time_comparison.png"
    )
    statistics_path = output_files.get(
        "processing_time_statistics", "processing_time_statistics.png"
    )
    summary_path = output_files.get(
        "processing_time_summary", "processing_time_summary.txt"
    )

    try:
        create_comparison_chart(
            question_numbers,
            model_a_times,
            model_b_times,
            output_file=comparison_path,
        )
        create_statistics_chart(
            question_numbers,
            model_a_times,
            model_b_times,
            output_file=statistics_path,
        )
        create_summary_table(
            question_numbers,
            model_a_times,
            model_b_times,
            output_file=summary_path,
        )
        log_success("å‡¦ç†æ™‚é–“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        log_info(f"â±ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}", indent=1)
        log_info(f"ğŸ–¼  æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ: {comparison_path}", indent=1)
        log_info(f"ğŸ–¼  çµ±è¨ˆãƒãƒ£ãƒ¼ãƒˆ: {statistics_path}", indent=1)
        log_info(f"ğŸ“ ã‚µãƒãƒªãƒ¼: {summary_path}", indent=1)
    except Exception as exc:
        log_warning(f"å‡¦ç†æ™‚é–“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")


def main():
    """
    Main entry point for the script.
    """
    output_files = get_output_file_names()
    default_time_log = output_files.get(
        "processing_time_log", "processing_time_log.txt"
    )

    parser = argparse.ArgumentParser(
        description="Collect LLM responses from API for evaluation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Collect responses from questions.txt
    python collect_responses.py questions.txt -o responses.csv
    
    # Use custom API URL
    python collect_responses.py questions.txt --api-url http://localhost:8080/api/v1/urls
    
    # Use custom models
    python collect_responses.py questions.txt --model-a claude3.5-sonnet --model-b claude4.5-haiku
    
    # Use custom identity
    python collect_responses.py questions.txt --identity YOUR_IDENTITY

Input file format:
    - Text file (.txt): One question per line. Lines starting with # are treated as comments.
    - CSV file (.csv): First column contains questions (with or without header row)
    
    Text file example:
        AIã‚ªãƒšå®¤ã®ç›¸è«‡çª“å£ã¯ã©ã“ï¼Ÿ
        ä¼šç¤¾ã®ä¼‘æš‡åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
        # This is a comment
        ç¤¾å†…ã®WiFiãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯ï¼Ÿ
    
    CSV file example:
        Questions
        AIã‚ªãƒšå®¤ã®ç›¸è«‡çª“å£ã¯ã©ã“ï¼Ÿ
        ä¼šç¤¾ã®ä¼‘æš‡åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
        """,
    )

    parser.add_argument(
        "input_file",
        help="Path to the input file containing questions (.txt or .csv format)",
    )

    parser.add_argument(
        "-o",
        "--output",
        default="collected_responses.csv",
        help="Path to the output CSV file (default: collected_responses.csv)",
    )

    parser.add_argument(
        "--api-url",
        default="http://0.0.0.0:8080/api/v2/questions",
        help="API endpoint URL (default: http://0.0.0.0:8080/api/v2/questions)",
    )

    parser.add_argument(
        "--model-a",
        default="claude3.5-sonnet",
        help="Model name for Model A (default: claude3.5-sonnet)",
    )

    parser.add_argument(
        "--model-b",
        default="claude4.5-haiku",
        help="Model name for Model B (default: claude4.5-haiku)",
    )

    parser.add_argument(
        "--identity",
        default=None,
        help=f"x-amzn-oidc-identity header value (default: from config, currently {get_default_identity()})",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=None,
        help=f"Request timeout in seconds (default: from config, currently {get_timeout()})",
    )

    parser.add_argument(
        "--delay",
        type=float,
        default=None,
        help=f"Delay between API calls in seconds (default: from config, currently {get_api_delay()})",
    )

    parser.add_argument(
        "--time-log",
        default=None,
        help=f"Path to processing time log file (default: {default_time_log})",
    )

    args = parser.parse_args()

    log_section("LLM Response Collector")

    # Read questions
    log_info(f"\nReading questions from: {args.input_file}")
    questions = read_questions(args.input_file)
    log_success(f"Loaded {len(questions)} questions")

    if len(questions) == 0:
        log_error("No questions found in input file.")
        sys.exit(1)

    # Collect responses
    time_log_path = args.time_log or default_time_log

    df = collect_responses(
        questions=questions,
        api_url=args.api_url,
        model_a=args.model_a,
        model_b=args.model_b,
        identity=args.identity if args.identity else None,
        timeout=args.timeout if args.timeout else None,
        delay=args.delay if args.delay else None,
        verbose=True,
        time_log_path=time_log_path,
    )

    # Save to CSV
    log_section("ğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")
    df.to_csv(args.output, index=False, quoting=csv.QUOTE_ALL)

    log_success(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {args.output}")
    log_info(f"è¡Œæ•°: {len(df)}", indent=1)
    log_info(f"åˆ—æ•°: {len(df.columns)}", indent=1)
    log_info(f"åˆ—å: {', '.join(df.columns)}", indent=1)

    # Check for errors
    failed_a = df[df["Model_A_Response"] == ""].shape[0]
    failed_b = df[df["Model_B_Response"] == ""].shape[0]

    log_section("âœ… åé›†å®Œäº†!")
    log_info(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output}")
    log_info(f"ğŸ“Š åé›†ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°: {len(df)}")

    if failed_a > 0 or failed_b > 0:
        log_warning("\nè­¦å‘Š:")
        if failed_a > 0:
            log_error(
                f"Model A ({args.model_a}): {failed_a}ä»¶ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã«å¤±æ•—",
                indent=1,
            )
        if failed_b > 0:
            log_error(
                f"Model B ({args.model_b}): {failed_b}ä»¶ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã«å¤±æ•—",
                indent=1,
            )
    else:
        log_success("\nã™ã¹ã¦ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£å¸¸ã«å–å¾—ã•ã‚Œã¾ã—ãŸ!")

    log_section("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    log_info("è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ:")
    log_info(f"  python llm_judge_evaluator.py {args.output} -n 5", indent=1)
    log_info("\nã¾ãŸã¯:")
    log_info(f"  python ragas_llm_judge_evaluator.py {args.output} -n 5", indent=1)


if __name__ == "__main__":
    main()
