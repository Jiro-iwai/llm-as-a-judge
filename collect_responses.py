#!/usr/bin/env python3
"""
LLM Response Collector Script

This script collects responses from the LLM API for two models (claude3.5-sonnet and claude4.5-haiku)
and creates a CSV file suitable for evaluation.

Usage:
    python collect_responses.py questions.txt -o output.csv
    python collect_responses.py questions.txt --api-url http://localhost:8080/api/v1/urls
"""

import argparse
import csv
import json
import re
import sys
import time
import uuid
from typing import List, Optional

import pandas as pd
import requests
from tqdm import tqdm


def clean_html(text_block: str) -> str:
    """
    Replaces HTML links with a 'Text (URL)' format and removes other HTML tags.
    """
    if not text_block:
        return ""

    # Pattern to find <a href="...">...</a> and capture the URL and the link text.
    link_pattern = re.compile(r'<a.*?href="([^"]*)".*?>(.*?)</a>', re.DOTALL)
    cleaned_text = link_pattern.sub(r"\2 (\1)", text_block)

    # Pattern to find and remove any other leftover HTML tags.
    tag_pattern = re.compile(r"<[^>]+>")
    cleaned_text = tag_pattern.sub("", cleaned_text)

    # Normalize whitespace for better readability.
    cleaned_text = re.sub(r"[\r\n]{2,}", "\n\n", cleaned_text).strip()

    return cleaned_text


def clean_and_format_llm_log(messy_text: str) -> str:
    """
    Parses, cleans, and formats a messy LLM log string, ensuring no sections are lost.
    This function is adapted from ../log-output-simplifier/main.py

    Args:
        messy_text: The raw JSON string from the LLM log file or the answer field content.

    Returns:
        A clean, formatted string with all available sections included.
    """
    try:
        # Try to parse as JSON first
        log_data = json.loads(messy_text)
        # The main content is usually inside the "answer" key.
        answer_content = log_data.get("answer", messy_text)
        # Normalize escaped newlines to actual newlines.
        answer_content = answer_content.replace("\\n", "\n")
    except json.JSONDecodeError:
        # If not JSON, treat as plain text
        answer_content = messy_text.replace("\\n", "\n")
    except Exception as e:
        return f"An unexpected error occurred during JSON parsing: {e}"

    # Define all sections we want to extract from the log.
    sections_to_find = [
        {"title": "## ğŸ“ Task ã‚¿ã‚¹ã‚¯", "marker": "ã‚¿ã‚¹ã‚¯ï¼š"},
        {"title": "## ğŸ’¬ Reaction åå¿œ", "marker": "åå¿œï¼š"},
        {"title": "## ğŸ“‚ Classification åˆ†é¡", "marker": "åˆ†é¡ï¼š"},
        {"title": "## ğŸ“Š Status çŠ¶æ…‹", "marker": "çŠ¶æ…‹ï¼š"},
        {"title": "## ğŸ¤– LLM Thought Process æ€è€ƒ", "marker": "æ€è€ƒï¼š"},
        {"title": "## âš¡ Action è¡Œå‹•", "marker": "è¡Œå‹•ï¼š"},
        {"title": "## âŒ¨ï¸ Action Input è¡Œå‹•å…¥åŠ›", "marker": "è¡Œå‹•å…¥åŠ›ï¼š"},
        {"title": "## ğŸ“š Raw Search Results (Cleaned) è¦³å¯Ÿ", "marker": "è¦³å¯Ÿï¼š"},
        {"title": "## âœ… Final Answer å›ç­”", "marker": "å›ç­”ï¼š"},
        {"title": "## ğŸ”— URLs URL", "marker": "URLï¼š"},
    ]

    # Find the starting position of each section marker in the text.
    found_sections = []
    for section in sections_to_find:
        # Use a loop to find all occurrences of a marker (like 'æ€è€ƒï¼š')
        start_index = -1
        while True:
            start_index = answer_content.find(section["marker"], start_index + 1)
            if start_index == -1:
                break
            found_sections.append(
                {
                    "start": start_index,
                    "title": section["title"],
                    "marker_len": len(section["marker"]),
                }
            )

    # If no markers are found, clean the entire text as a fallback.
    if not found_sections:
        return (
            "No known section markers found. Performing a full clean:\n\n"
            + clean_html(answer_content)
        )

    # Sort the found sections by their starting position to process them in order.
    found_sections.sort(key=lambda x: x["start"])

    output_parts = []
    # Extract the content for each section.
    for i, section in enumerate(found_sections):
        content_start = section["start"] + section["marker_len"]

        # Determine the end of the current section's content.
        # It's either the start of the next section or the end of the string.
        if i + 1 < len(found_sections):
            content_end = found_sections[i + 1]["start"]
        else:
            # For the last section, go all the way to the end of the content.
            content_end = len(answer_content)

        content = answer_content[content_start:content_end]

        # Clean the extracted content block.
        cleaned_content = clean_html(content)

        # Avoid adding empty sections
        if not cleaned_content.strip():
            continue

        output_parts.append(section["title"])
        output_parts.append("---")

        # Special formatting for the "Observation" (Raw Search Results) section.
        if "Raw Search Results" in section["title"]:
            results = cleaned_content.split(
                "################################################"
            )
            for j, result in enumerate(results, 1):
                if result.strip():
                    output_parts.append(f"### Result {j}\n{result.strip()}")
        else:
            output_parts.append(cleaned_content.strip())

        output_parts.append("\n")

    return "\n".join(output_parts)


def format_response(response_text: str) -> str:
    """
    Format the API response using the log simplifier.

    Args:
        response_text: The raw response text from API (answer field content)

    Returns:
        Formatted response string
    """
    if not response_text:
        return ""

    # Wrap the response in a JSON-like structure for the formatter
    # The formatter expects JSON with an "answer" field, but we already have the answer content
    try:
        # Try to format as if it's already the answer content
        formatted = clean_and_format_llm_log(response_text)
        return formatted
    except Exception as e:
        # If formatting fails, return original text
        print(f"  âš ï¸  ãƒ­ã‚°æ•´å½¢ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return response_text


def call_api(
    question: str,
    api_url: str,
    model_name: str,
    identity: str = "A14804",
    timeout: int = 120,
    verbose: bool = True,
) -> Optional[str]:
    """
    Call the LLM API and return the response.

    Args:
        question: The question to ask
        api_url: The API endpoint URL
        model_name: The model name (claude3.5-sonnet or claude4.5-haiku)
        identity: The x-amzn-oidc-identity header value
        timeout: Request timeout in seconds
        verbose: Whether to print detailed logs

    Returns:
        The response text, or None if failed
    """
    question_uuid = str(uuid.uuid4())

    # Prepare the request
    url = f"{api_url}?llm_model_name={model_name}&rag_enabled=auto"
    headers = {"x-amzn-oidc-identity": identity, "Content-Type": "application/json"}
    data = {
        "question_uuid": question_uuid,
        "messages": [{"role": "user", "content": question}],
    }

    if verbose:
        print(f"  ğŸ“¤ [{model_name}] APIå‘¼ã³å‡ºã—é–‹å§‹")
        print(f"     URL: {url}")
        print(
            f"     è³ªå•: {question[:60]}..."
            if len(question) > 60
            else f"     è³ªå•: {question}"
        )

    start_time = time.time()
    response: Optional[requests.Response] = None

    try:
        response = requests.post(url, headers=headers, json=data, timeout=timeout)
        elapsed_time = time.time() - start_time

        if verbose:
            print(
                f"  ğŸ“¥ [{model_name}] HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code} (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)"
            )

        response.raise_for_status()

        # Parse response - API returns object format: {"answer": "...", "urls": [...], ...}
        response_data = response.json()

        # Handle both array and object formats
        if isinstance(response_data, list) and len(response_data) >= 1:
            # Array format: ["{...}", status_code]
            json_str = response_data[0]
            if isinstance(json_str, str):
                parsed = json.loads(json_str)
            else:
                parsed = json_str
        elif isinstance(response_data, dict):
            # Direct object format: {"answer": "...", ...}
            parsed = response_data
        else:
            if verbose:
                print(f"  âš ï¸  [{model_name}] äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼")
            return response.text

        # Extract the answer field (this contains the LLM response)
        if "answer" in parsed:
            answer = parsed["answer"]
            answer_length = len(answer)
            if verbose:
                print(
                    f"  âœ… [{model_name}] ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—æˆåŠŸ (answeré•·ã•: {answer_length:,}æ–‡å­—)"
                )
                if "urls" in parsed and isinstance(parsed["urls"], list):
                    print(f"     æ¤œç´¢çµæœURLæ•°: {len(parsed['urls'])}")
            return answer
        else:
            if verbose:
                print(f"  âš ï¸  [{model_name}] 'answer'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return response.text

    except requests.exceptions.Timeout:
        elapsed_time = time.time() - start_time
        print(
            f"\n  âŒ [{model_name}] APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            file=sys.stderr,
        )
        print(f"     URL: {url}", file=sys.stderr)
        return None
    except requests.exceptions.RequestException as e:
        elapsed_time = time.time() - start_time
        print(
            f"\n  âŒ [{model_name}] APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            file=sys.stderr,
        )
        print(f"     ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        print(f"     URL: {url}", file=sys.stderr)
        return None
    except (json.JSONDecodeError, KeyError) as e:
        elapsed_time = time.time() - start_time
        print(
            f"\n  âŒ [{model_name}] ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼ (çµŒéæ™‚é–“: {elapsed_time:.2f}ç§’)",
            file=sys.stderr,
        )
        print(f"     ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        response_text: Optional[str] = None
        if response is not None:
            try:
                response_text = (
                    response.text[:200]
                    if hasattr(response, "text")
                    else str(response)[:200]
                )
                print(f"     ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {response_text}", file=sys.stderr)
            except AttributeError:
                pass
        return response_text


def collect_responses(
    questions: List[str],
    api_url: str,
    model_a: str,
    model_b: str,
    identity: str = "A14804",
    timeout: int = 120,
    delay: float = 1.0,
    verbose: bool = True,
) -> pd.DataFrame:
    """
    Collect responses from both models for all questions.

    Args:
        questions: List of questions to ask
        api_url: The API endpoint URL
        model_a: Model name for Model A (e.g., claude3.5-sonnet)
        model_b: Model name for Model B (e.g., claude4.5-haiku)
        identity: The x-amzn-oidc-identity header value
        timeout: Request timeout in seconds
        delay: Delay between API calls in seconds
        verbose: Whether to print detailed logs

    Returns:
        DataFrame with Question, Model_A_Response, Model_B_Response columns
        (Each response contains only the "answer" field from API)
    """
    results = []
    total_start_time = time.time()

    print("=" * 70)
    print("ğŸ“‹ åé›†è¨­å®š")
    print("=" * 70)
    print(f"  è³ªå•æ•°: {len(questions)}")
    print(f"  Model A: {model_a}")
    print(f"  Model B: {model_b}")
    print(f"  API URL: {api_url}")
    print(f"  ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {delay}ç§’")
    print(f"  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")
    print(
        f"  äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„{len(questions) * 2 * (delay + 15):.0f}ç§’ (å„APIå‘¼ã³å‡ºã—15ç§’æƒ³å®š)"
    )
    print("=" * 70)
    print()

    success_count_a = 0
    success_count_b = 0
    failed_count_a = 0
    failed_count_b = 0

    for idx, question in enumerate(tqdm(questions, desc="ğŸ“Š é€²æ—"), 1):
        if verbose:
            print(f"\n{'=' * 70}")
            print(f"ğŸ“ è³ªå• {idx}/{len(questions)}")
            print(f"{'=' * 70}")
            print(f"è³ªå•: {question}")
            print()

        question_start_time = time.time()

        # Call Model A
        if verbose:
            print(f"[{idx}/{len(questions)}] Model A ({model_a}) ã‚’å‘¼ã³å‡ºã—ä¸­...")
        response_a_raw = call_api(
            question, api_url, model_a, identity, timeout, verbose=verbose
        )

        # Format the response using log simplifier
        if response_a_raw:
            if verbose:
                print("  ğŸ”§ Model A ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢ä¸­...")
            response_a = format_response(response_a_raw)
            success_count_a += 1
        else:
            response_a = ""
            failed_count_a += 1

        # Wait between Model A and Model B calls
        if verbose:
            print(f"  â¸ï¸  Model Bå‘¼ã³å‡ºã—ã¾ã§{delay}ç§’å¾…æ©Ÿä¸­...")
        time.sleep(delay)  # Rate limiting

        # Call Model B
        if verbose:
            print(f"[{idx}/{len(questions)}] Model B ({model_b}) ã‚’å‘¼ã³å‡ºã—ä¸­...")
        response_b_raw = call_api(
            question, api_url, model_b, identity, timeout, verbose=verbose
        )

        # Format the response using log simplifier
        if response_b_raw:
            if verbose:
                print("  ğŸ”§ Model B ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢ä¸­...")
            response_b = format_response(response_b_raw)
            success_count_b += 1
        else:
            response_b = ""
            failed_count_b += 1

        # Store formatted responses
        # Column names compatible with both llm_judge_evaluator.py and ragas_llm_judge_evaluator.py
        results.append(
            {
                "Question": question,
                "Model_A_Response": response_a,
                "Model_B_Response": response_b,
            }
        )

        question_elapsed = time.time() - question_start_time

        if verbose:
            status_a = "âœ…" if response_a else "âŒ"
            status_b = "âœ…" if response_b else "âŒ"
            print(f"\n  ğŸ“Š è³ªå• {idx} å®Œäº† (çµŒéæ™‚é–“: {question_elapsed:.2f}ç§’)")
            print(f"     Model A: {status_a} | Model B: {status_b}")
            print(f"     æˆåŠŸæ•°: A={success_count_a}/{idx}, B={success_count_b}/{idx}")

        # Wait before next question (if not the last question)
        if idx < len(questions):
            if verbose:
                print(f"  â¸ï¸  æ¬¡ã®è³ªå•ã¾ã§{delay}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(delay)  # Rate limiting

    total_elapsed = time.time() - total_start_time

    if verbose:
        print("\n" + "=" * 70)
        print("ğŸ“Š åé›†å®Œäº†çµ±è¨ˆ")
        print("=" * 70)
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’ ({total_elapsed / 60:.2f}åˆ†)")
        print(f"  è³ªå•æ•°: {len(questions)}")
        print(f"  Model A ({model_a}):")
        print(
            f"    âœ… æˆåŠŸ: {success_count_a}/{len(questions)} ({success_count_a / len(questions) * 100:.1f}%)"
        )
        print(f"    âŒ å¤±æ•—: {failed_count_a}/{len(questions)}")
        print(f"  Model B ({model_b}):")
        print(
            f"    âœ… æˆåŠŸ: {success_count_b}/{len(questions)} ({success_count_b / len(questions) * 100:.1f}%)"
        )
        print(f"    âŒ å¤±æ•—: {failed_count_b}/{len(questions)}")
        print("=" * 70)

    return pd.DataFrame(results)


def read_questions(input_file: str) -> List[str]:
    """
    Read questions from a text file or CSV file.

    Supports:
    - Text file: One question per line
    - CSV file: First column contains questions (with or without header)

    Args:
        input_file: Path to the input file

    Returns:
        List of questions
    """
    questions = []
    try:
        # Check if file is CSV by extension or try to read as CSV first
        if input_file.lower().endswith(".csv"):
            # Try to read as CSV
            df = pd.read_csv(input_file)
            # Get first column (usually "Questions" or "Question")
            first_col = df.columns[0]
            questions = df[first_col].dropna().astype(str).tolist()
            # Remove header if it looks like a header (common header names)
            if questions and questions[0].lower() in [
                "question",
                "questions",
                "q",
                "query",
                "queries",
            ]:
                questions = questions[1:]
            # Filter out empty strings
            questions = [q.strip() for q in questions if q.strip()]
        else:
            # Read as text file (one question per line)
            with open(input_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith(
                        "#"
                    ):  # Skip empty lines and comments
                        questions.append(line)
    except FileNotFoundError:
        print(f"ERROR: Input file '{input_file}' not found.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: Failed to read input file: {e}", file=sys.stderr)
        sys.exit(1)

    return questions


def main():
    """
    Main entry point for the script.
    """
    parser = argparse.ArgumentParser(
        description="Collect LLM responses from API for evaluation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Collect responses from questions.txt
    python collect_responses.py questions.txt -o responses.csv
    
    # Use custom API URL
    python collect_responses.py questions.txt --api-url http://localhost:8080/api/v1/urls
    
    # Use custom models
    python collect_responses.py questions.txt --model-a claude3.5-sonnet --model-b claude4.5-haiku
    
    # Use custom identity
    python collect_responses.py questions.txt --identity YOUR_IDENTITY

Input file format:
    - Text file (.txt): One question per line. Lines starting with # are treated as comments.
    - CSV file (.csv): First column contains questions (with or without header row)
    
    Text file example:
        AIã‚ªãƒšå®¤ã®ç›¸è«‡çª“å£ã¯ã©ã“ï¼Ÿ
        ä¼šç¤¾ã®ä¼‘æš‡åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
        # This is a comment
        ç¤¾å†…ã®WiFiãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯ï¼Ÿ
    
    CSV file example:
        Questions
        AIã‚ªãƒšå®¤ã®ç›¸è«‡çª“å£ã¯ã©ã“ï¼Ÿ
        ä¼šç¤¾ã®ä¼‘æš‡åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
        """,
    )

    parser.add_argument(
        "input_file",
        help="Path to the input file containing questions (.txt or .csv format)",
    )

    parser.add_argument(
        "-o",
        "--output",
        default="collected_responses.csv",
        help="Path to the output CSV file (default: collected_responses.csv)",
    )

    parser.add_argument(
        "--api-url",
        default="http://0.0.0.0:8080/api/v2/questions",
        help="API endpoint URL (default: http://0.0.0.0:8080/api/v2/questions)",
    )

    parser.add_argument(
        "--model-a",
        default="claude3.5-sonnet",
        help="Model name for Model A (default: claude3.5-sonnet)",
    )

    parser.add_argument(
        "--model-b",
        default="claude4.5-haiku",
        help="Model name for Model B (default: claude4.5-haiku)",
    )

    parser.add_argument(
        "--identity",
        default="A14804",
        help="x-amzn-oidc-identity header value (default: A14804)",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=120,
        help="Request timeout in seconds (default: 120)",
    )

    parser.add_argument(
        "--delay",
        type=float,
        default=1.0,
        help="Delay between API calls in seconds (default: 1.0)",
    )

    args = parser.parse_args()

    print("=" * 70)
    print("LLM Response Collector")
    print("=" * 70)

    # Read questions
    print(f"\nReading questions from: {args.input_file}")
    questions = read_questions(args.input_file)
    print(f"âœ“ Loaded {len(questions)} questions")

    if len(questions) == 0:
        print("ERROR: No questions found in input file.", file=sys.stderr)
        sys.exit(1)

    # Collect responses
    df = collect_responses(
        questions=questions,
        api_url=args.api_url,
        model_a=args.model_a,
        model_b=args.model_b,
        identity=args.identity,
        timeout=args.timeout,
        delay=args.delay,
        verbose=True,
    )

    # Save to CSV
    print("\n" + "=" * 70)
    print("ğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")
    print("=" * 70)
    df.to_csv(args.output, index=False, quoting=csv.QUOTE_ALL)

    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {args.output}")
    print(f"   è¡Œæ•°: {len(df)}")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   åˆ—å: {', '.join(df.columns)}")

    # Check for errors
    failed_a = df[df["Model_A_Response"] == ""].shape[0]
    failed_b = df[df["Model_B_Response"] == ""].shape[0]

    print("\n" + "=" * 70)
    print("âœ… åé›†å®Œäº†!")
    print("=" * 70)
    print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output}")
    print(f"ğŸ“Š åé›†ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°: {len(df)}")

    if failed_a > 0 or failed_b > 0:
        print("\nâš ï¸  è­¦å‘Š:")
        if failed_a > 0:
            print(f"  âŒ Model A ({args.model_a}): {failed_a}ä»¶ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã«å¤±æ•—")
        if failed_b > 0:
            print(f"  âŒ Model B ({args.model_b}): {failed_b}ä»¶ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã«å¤±æ•—")
    else:
        print("\nâœ… ã™ã¹ã¦ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£å¸¸ã«å–å¾—ã•ã‚Œã¾ã—ãŸ!")

    print("\n" + "=" * 70)
    print("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 70)
    print("è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ:")
    print(f"  python llm_judge_evaluator.py {args.output} -n 5")
    print("\nã¾ãŸã¯:")
    print(f"  python ragas_llm_judge_evaluator.py {args.output} -n 5")
    print("=" * 70)


if __name__ == "__main__":
    main()
